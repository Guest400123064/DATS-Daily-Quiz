{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATS Daily Quiz Session One: ML-Basics\n",
    "\n",
    "[Quiz Source](https://www.1point3acres.com/bbs/thread-713903-1-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1, 03-25-2021\n",
    "\n",
    "**Question:** Explain the concepts of *Overfitting* and *Underfitting*\n",
    "\n",
    "**Ans from Web:** \n",
    "* *Overfitting:* Overfitting refers to a model that models the training data too well. Overfitting happens when a model **learns the detail and noise in the training data** to the extent that it negatively impacts the performance of the model on new data (test set). This means that the **noise or random fluctuations in the training data is picked up and learned as concepts by the model.** The problem is that **these concepts DO NOT apply to new data** and negatively impact the models **ability to generalize.** Overfitting is more likely with nonparametric and nonlinear models that **have more flexibility** when learning a target function. As such, many nonparametric machine learning algorithms also include parameters or techniques to **limit and constrain how much detail the model learns.** For example, decision trees are a nonparametric machine learning algorithm that is very flexible and is subject to overfitting training data. This problem can be addressed by **pruning a tree** after it has learned in order to remove some of the detail it has picked up.\n",
    "* *Underfitting:* Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
    "An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data.\n",
    "Underfitting is often not discussed as it is easy to detect given a good performance metric. The remedy is to move on and try alternate machine learning algorithms. Nevertheless, it does provide a good contrast to the problem of overfitting.\n",
    "\n",
    "**In a Nutshell:**\n",
    "* *Overfitting:* The resulting model has much higher performance on the training set than on the test set, as it learns ungeneralizable details/noises as \"concepts\" that belong to the training set only. Flexible models, e.g. non-parametric models like decision trees, are usually prone to this problem.\n",
    "* *Underfitting:* Underfitting refers to a model that can neither model the training data nor generalize to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 2, 03-26-2021\n",
    "\n",
    "**Question:** Explain the notion of *Variance-Bias Trade-off*\n",
    "\n",
    "**Ans from Web:** Please refer to the [wiki](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).\n",
    "\n",
    "**In a Nutshell:**\n",
    "\n",
    "The *Variance-Bias Trade-off* often refers to the *Model Complexity-Generalizability Trade-off*. Intuitively, complex models or **High Variance** models, such as neural network families, tend to overfit the training data while simple model or **High Bias** models like linear regression tend to underfit the training data, and conceptually we have: \n",
    "\n",
    "$$E[Loss] = Bias[\\hat{f}] + Var[\\hat{f}] + \\epsilon\\ (random\\ noise)$$\n",
    "    \n",
    "So usually bias and variance are not compatible with each other. Note that the \"complexity\" here does not necessarily means the number of parameters or the complexity of model architecture. Rather, we'd better think of (1) the amount of assumptions we impose on our algorithm and (2) how well our assumptions fit the atual situations. Therefore, during the modeling process, we seek \"just the right amount of complexity\", trying to minimize bias and variance at the same time as a dual objective problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 3, 03-27-2021\n",
    "\n",
    "**Question:** How to prevent *Overfitting* problems?\n",
    "\n",
    "**Ans from Web:** Can be found [here](https://www.zhihu.com/question/59201590/answer/167392763).\n",
    "                  Regularization can be found [here](https://blog.csdn.net/jinping_shi/article/details/52433975).\n",
    "\n",
    "**In a Nutshell:**\n",
    "\n",
    "Usually we can prevent overfitting through 3 kinds of ideas: \n",
    "\n",
    "* Increase the amount of data. As long as data is given to allow the model to \"see\" as many \"exceptions\" as possible, it will continue to modify itself to get better results. \n",
    "\n",
    "* Regularization. Regularization is a function of adding parameters to the loss function, the more parameters or larger, the larger the loss function, that is, to punish the behavior of increasing the model parameters, so that the model will not recklessly increase the parameters, it can reduce overfitting. l2 regularization mathematical principle is that it adds a regularization factor to the penalty function, which leads to each iteration, the parameters are multiplied by a factor less than 1, so the overall parameters are getting smaller. Therefore the overall parameters are getting smaller.\n",
    "\n",
    "* Model Ensemble. This is because the expected error of randomly selecting one of the N models as the output will be larger than the average output error of all models. Therefore, we can consider methods such as bagging and boosting to prevent overfitting."
   ]
  },
  {
   "source": [
    "## Day 4, 03-28-2021\n",
    "\n",
    "**N/A**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Day 5, 03-29-2021\n",
    "\n",
    "**Question:** What are the differences between *Generative* and *Discriminative* models?\n",
    "\n",
    "**Ans from Web:**\n",
    "\n",
    "**In a Nutshell:**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}